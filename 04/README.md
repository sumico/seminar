page 1
![image](https://github.com/user-attachments/assets/6c3559e1-a44a-44f1-a42e-216e14920489)

---
page 2
![image](https://github.com/user-attachments/assets/ad169263-7434-4b03-b146-a1f16ba2dfdd)

---
page 3
![image](https://github.com/user-attachments/assets/265a2957-6ca4-422f-8b71-898130365eea)

Model distillation is the process of training a new model based on the input-output pairs of an existing model to create a model that is very similar to the existing model. 
Also, when the structure of an existing model is obvious, a model that is retrained using different data is called a derived model.

Source: https://zero2one.jp/ai-word/distillation/?srsltid=AfmBOooxxnXh3GeNpa0s-IlvvBcHAktitAxz0S1UCr3FCrONFARnjeot

---

